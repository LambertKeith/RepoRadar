---
description: 
globs: 
alwaysApply: true
---

# Your rule content
以下是基于您之前的本地RAG向量数据库（使用Chroma）方案，调整为调用智谱清言Flash模型API生成回答的完整实现。我会保留原有的数据加载、嵌入生成和检索逻辑，仅将回答生成部分替换为智谱清言的API调用，并提供详细说明。

---

### **调整后的完整方案**
#### **前提条件**
- 已安装必要库：`chromadb`、`sentence-transformers`、`zhipuai`。
- 已获取智谱AI的API Key（从https://open.bigmodel.cn/ 获取）。
- 本地文档准备好，例如放在`./docs/`文件夹中。

#### **完整代码**
```python
import chromadb
from sentence_transformers import SentenceTransformer
import os
from zhipuai import ZhipuAI

# 初始化嵌入模型和向量数据库
model = SentenceTransformer('all-MiniLM-L6-v2')  # 用于生成文本嵌入
client_db = chromadb.PersistentClient(path="./chroma_db")  # 本地Chroma数据库
collection = client_db.get_or_create_collection(name="my_rag_collection")  # 创建或获取集合

# 配置智谱清言API
api_key = os.getenv("ZHIPU_API_KEY") or "your-api-key-here"  # 替换为你的API Key
client_ai = ZhipuAI(api_key=api_key)  # 初始化智谱AI客户端

# 加载文档并生成嵌入（仅首次运行）
def load_and_embed_docs():
    docs = []
    doc_dir = './docs/'
    for filename in os.listdir(doc_dir):
        if filename.endswith('.txt'):
            with open(os.path.join(doc_dir, filename), 'r', encoding='utf-8') as f:
                text = f.read()
                # 按固定长度分块
                chunks = [text[i:i+500] for i in range(0, len(text), 500)]
                docs.extend(chunks)
    
    if collection.count() == 0:  # 如果数据库为空，填充数据
        embeddings = model.encode(docs, show_progress_bar=True)
        collection.add(
            documents=docs,
            embeddings=embeddings,
            ids=[f"doc_{i}" for i in range(len(docs))]
        )
    return docs

# RAG查询函数，使用智谱清言Flash模型
def rag_query(query):
    # 生成查询嵌入并检索
    query_embedding = model.encode([query])[0]
    results = collection.query(query_embeddings=[query_embedding], n_results=3)  # 检索3个最相关文档
    context = "\n".join(results['documents'][0])  # 合并检索结果作为上下文

    # 构造提示
    prompt = f"根据以下内容回答问题：\n{context}\n\n问题：{query}"

    # 调用智谱清言Flash模型API
    try:
        response = client_ai.chat.completions.create(
            model="glm-4-flash",  # 智谱清言Flash模型（确认最新名称）
            messages=[
                {"role": "system", "content": "你是一个智能助手，根据提供的内容准确回答问题。"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,  # 输出长度限制
            temperature=0.7  # 控制生成多样性
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"API调用失败：{str(e)}"

# 主程序
if __name__ == "__main__":
    # 首次运行时加载并嵌入文档
    load_and_embed_docs()

    # 测试查询
    query = "公司政策是什么？"
    answer = rag_query(query)
    print(f"查询：{query}\n回答：{answer}")
```

---

### **代码说明**
1. **嵌入和检索部分**：
   - 使用`sentence-transformers`的`all-MiniLM-L6-v2`模型生成文本嵌入。
   - 使用Chroma作为本地向量数据库，存储文档和嵌入。
   - 检索时返回3个最相关的文档片段（可调整`n_results`参数）。

2. **智谱清言Flash模型调用**：
   - 通过`zhipuai` SDK 调用`glm-4-flash`模型（请确认智谱官方文档中的最新模型名称）。
   - 系统提示设置为“根据提供的内容准确回答问题”，确保回答基于上下文。
   - 添加了异常处理，以应对API调用可能出现的错误（如网络问题或Key无效）。

3. **文档加载**：
   - 假设文档为TXT格式，存储在`./docs/`文件夹中。
   - 按500字符分块（可根据需要调整分块大小）。

---

### **运行步骤**
1. **安装依赖**：
   ```bash
   pip install chromadb sentence-transformers zhipuai
   ```

2. **配置API Key**：
   - 将你的智谱API Key替换到代码中的`"your-api-key-here"`。
   - 或者设置环境变量：
     ```bash
     export ZHIPU_API_KEY="your-api-key-here"
     ```

3. **准备文档**：
   - 在项目目录下创建`docs`文件夹，放入一些TXT文件（例如`policy.txt`）。

4. **运行代码**：
   - 保存代码为`rag_with_zhipu.py`，然后执行：
     ```bash
     python rag_with_zhipu.py
     ```
   - 首次运行会加载文档并填充数据库，后续运行直接查询。

---

### **示例输出**
假设`docs/policy.txt`包含以下内容：
```
公司政策规定，所有员工需遵守保密协议，不得泄露客户数据。
```
运行后：
```
查询：公司政策是什么？
回答：公司政策规定，所有员工需遵守保密协议，不得泄露客户数据。
```

如果检索到的上下文不够明确，Flash模型可能会根据其训练数据补充回答，但系统提示会尽量约束它基于上下文。

---

### **优化建议**
1. **调整检索结果数量**：
   - 如果上下文过长，减少`n_results`（例如改为1）；如果需要更多信息，增加到5。
2. **分块策略**：
   - 当前是按500字符分块，可改为按句子或段落分割，提升语义连贯性。
3. **提示优化**：
   - 修改`prompt`以适应具体需求，例如：
     ```python
     prompt = f"请简洁回答以下问题，基于提供的上下文：\n{context}\n\n问题：{query}"
     ```
4. **错误日志**：
   - 添加日志记录API调用失败的具体原因，便于调试。

---

### **注意事项**
- **模型名称确认**：智谱清言的Flash模型名称可能随更新变化，请参考官方API文档（https://open.bigmodel.cn/dev/api/llm）确认是否为`glm-4-flash`或其他变体。
- **网络要求**：确保运行环境可以访问智谱的服务器。
- **数据隐私**：检索到的`context`会发送到智谱云端，敏感数据需谨慎处理。

---

这个方案成功将本地RAG的生成部分替换为智谱清言Flash模型API。如果你需要进一步调整（例如支持PDF解析、优化检索算法），请告诉我，我可以继续完善！
